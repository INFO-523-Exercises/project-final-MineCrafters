[
  {
    "objectID": "src/MineCrafters_main.html",
    "href": "src/MineCrafters_main.html",
    "title": "Import Libraries and Load dataset",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nimport xgboost as xgb\n\n\n# Reading csv file\ndata = pd.read_csv('USA_Housing.csv')\ndata\n\n\n\n\n\n\n\n\nAvg. Area Income\nAvg. Area House Age\nAvg. Area Number of Rooms\nAvg. Area Number of Bedrooms\nArea Population\nPrice\nAddress\n\n\n\n\n0\n79545.458574\n5.682861\n7.009188\n4.09\n23086.800503\n1.059034e+06\n208 Michael Ferry Apt. 674\\nLaurabury, NE 3701...\n\n\n1\n79248.642455\n6.002900\n6.730821\n3.09\n40173.072174\n1.505891e+06\n188 Johnson Views Suite 079\\nLake Kathleen, CA...\n\n\n2\n61287.067179\n5.865890\n8.512727\n5.13\n36882.159400\n1.058988e+06\n9127 Elizabeth Stravenue\\nDanieltown, WI 06482...\n\n\n3\n63345.240046\n7.188236\n5.586729\n3.26\n34310.242831\n1.260617e+06\nUSS Barnett\\nFPO AP 44820\n\n\n4\n59982.197226\n5.040555\n7.839388\n4.23\n26354.109472\n6.309435e+05\nUSNS Raymond\\nFPO AE 09386\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4995\n60567.944140\n7.830362\n6.137356\n3.46\n22837.361035\n1.060194e+06\nUSNS Williams\\nFPO AP 30153-7653\n\n\n4996\n78491.275435\n6.999135\n6.576763\n4.02\n25616.115489\n1.482618e+06\nPSC 9258, Box 8489\\nAPO AA 42991-3352\n\n\n4997\n63390.686886\n7.250591\n4.805081\n2.13\n33266.145490\n1.030730e+06\n4215 Tracy Garden Suite 076\\nJoshualand, VA 01...\n\n\n4998\n68001.331235\n5.534388\n7.130144\n5.44\n42625.620156\n1.198657e+06\nUSS Wallace\\nFPO AE 73316\n\n\n4999\n65510.581804\n5.992305\n6.792336\n4.07\n46501.283803\n1.298950e+06\n37778 George Ridges Apt. 509\\nEast Holly, NV 2...\n\n\n\n\n5000 rows × 7 columns\n\n\n\n\nExploratory Data Analysis\n\ndata.info()#print a concise summary of a DataFrame\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5000 entries, 0 to 4999\nData columns (total 7 columns):\n #   Column                        Non-Null Count  Dtype  \n---  ------                        --------------  -----  \n 0   Avg. Area Income              5000 non-null   float64\n 1   Avg. Area House Age           5000 non-null   float64\n 2   Avg. Area Number of Rooms     5000 non-null   float64\n 3   Avg. Area Number of Bedrooms  5000 non-null   float64\n 4   Area Population               5000 non-null   float64\n 5   Price                         5000 non-null   float64\n 6   Address                       5000 non-null   object \ndtypes: float64(6), object(1)\nmemory usage: 273.6+ KB\n\n\n\n# Checking null values\ndata.isnull().sum()\n\nAvg. Area Income                0\nAvg. Area House Age             0\nAvg. Area Number of Rooms       0\nAvg. Area Number of Bedrooms    0\nArea Population                 0\nPrice                           0\nAddress                         0\ndtype: int64\n\n\n\ndata.describe()# Generate descriptive statistics summarizing central tendency, dispersion, and shape of dataset's distribution.\n\n\n\n\n\n\n\n\nAvg. Area Income\nAvg. Area House Age\nAvg. Area Number of Rooms\nAvg. Area Number of Bedrooms\nArea Population\nPrice\n\n\n\n\ncount\n5000.000000\n5000.000000\n5000.000000\n5000.000000\n5000.000000\n5.000000e+03\n\n\nmean\n68583.108984\n5.977222\n6.987792\n3.981330\n36163.516039\n1.232073e+06\n\n\nstd\n10657.991214\n0.991456\n1.005833\n1.234137\n9925.650114\n3.531176e+05\n\n\nmin\n17796.631190\n2.644304\n3.236194\n2.000000\n172.610686\n1.593866e+04\n\n\n25%\n61480.562388\n5.322283\n6.299250\n3.140000\n29403.928702\n9.975771e+05\n\n\n50%\n68804.286404\n5.970429\n7.002902\n4.050000\n36199.406689\n1.232669e+06\n\n\n75%\n75783.338666\n6.650808\n7.665871\n4.490000\n42861.290769\n1.471210e+06\n\n\nmax\n107701.748378\n9.519088\n10.759588\n6.500000\n69621.713378\n2.469066e+06\n\n\n\n\n\n\n\n\n# Data Claening\ndf = pd.DataFrame()\ndf['Income'] = data['Avg. Area Income'].round(2)# Round 'Avg. Area Income' to 2 decimal places and store in 'Income'.\ndf['House Age'] = data['Avg. Area House Age'].apply(int)# Converting 'Avg. Area House Age' to integers and adding it to 'df' as 'House Age'.\ndf['No. of Rooms'] = data['Avg. Area Number of Rooms'].apply(int)# Converting 'Avg. Area Number of Rooms' to integers and adding it to 'df' as 'No. of Rooms'.\ndf['No. of Bedrooms'] = data['Avg. Area Number of Bedrooms'].apply(int)# Converting 'Avg. Area Number of Bedrooms' to integers and adding it to 'df' as 'No. of Bedrooms'.\ndf['Population'] = data['Area Population'].apply(int)# Converting 'Area Population' to integers and adding it to 'df' as 'Population'.\ndf['Price'] = data['Price'].apply(int)# Converting 'Price' to integers and adding it to 'df' as 'Price'.\n\n\n# Price plot\nplt.figure(figsize = (12, 6), dpi = 200)\nsns.distplot(data.Price)\n\nC:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\seaborn\\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n\n\n\n\n\n# Price wrt Income\nplt.figure(figsize = (12, 6), dpi = 200)\nsns.lineplot(x = 'Income', y = 'Price', data = df)\n\n\n\n\n\n\n\n\n\n# Price wrt House Age\nplt.figure(figsize = (12, 6), dpi = 200)\nsns.barplot(x = 'House Age', y = 'Price', data = df, ci = None)\n\n\n\n\n\n\n\n\n\n# Price wrt No. of Rooms\nplt.figure(figsize = (12, 6), dpi = 200)\nsns.boxplot(x = 'No. of Rooms', y = 'Price', data = df)\n\n\n\n\n\n\n\n\n\n# Price wrt No. of Bedrooms\nplt.figure(figsize = (12, 6), dpi = 200)\nsns.barplot(x = 'No. of Bedrooms', y = 'Price', data = df, ci = None)\n\n\n\n\n\n\n\n\n\n# Price wrt Population\nplt.figure(figsize = (12, 6), dpi = 200)\nsns.scatterplot(x = 'Population', y = 'Price', data = df)\n\n\n\n\n\n\n\n\n\n#Heatmap showing correlation\nplt.figure(figsize = (12, 8), dpi = 200)\nsns.heatmap(data.corr(), annot = True, square = True)\n\n\n\n\n\n\n\n\n\n\nSetting Target and Variable and Train Test Split\n\n# Selecting features and target variable\nX = data.drop(['Price', 'Address'], axis=1)\ny = data['Price']\n\n\n# Splitting the dataset into training and testing sets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\nRandom FOrest Regressor\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n# Define the parameter grid for hyperparameter tuning\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [5, 10, 15, 20],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# Initialize the Random Forest model\nrf = RandomForestRegressor(random_state=42)\n\n# Setup GridSearchCV\ngrid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n\n# Fit the model to the scaled training data\ngrid_search.fit(X_train, y_train)\n\n# Get the best estimator\nbest_rf = grid_search.best_estimator_\n\n\n\nRandom Forest Model Evaluation\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Make predictions using the best model on the scaled test data\npredictions = best_rf.predict(X_test)\n\n# Calculate and print evaluation metrics\nmse = mean_squared_error(y_test, predictions)\nr2 = r2_score(y_test, predictions)\nprint(f'Mean Squared Error: {mse}')\nprint(f'R-squared: {r2}')\nprint('Best Parameters:', grid_search.best_params_)\n\nMean Squared Error: 14308544382.476547\nR-squared: 0.8837010564891862\nBest Parameters: {'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 300}\n\n\n\ny_test\n\n1501    1.339096e+06\n2586    1.251794e+06\n2653    1.340095e+06\n1055    1.431508e+06\n705     1.042374e+06\n            ...     \n4711    1.107031e+06\n2313    1.405505e+06\n3214    1.924156e+06\n2732    1.571254e+06\n1926    8.831475e+05\nName: Price, Length: 1000, dtype: float64\n\n\n\npredictions\n\narray([1307087.4879016 , 1244017.24023052, 1275777.07066763,\n       1214577.43025003, 1192457.00400717, 1539732.09473113,\n       1054950.89065146,  844769.54643916,  872790.21628404,\n       1398049.16235094,  661831.05833939, 1301529.31323965,\n       1016518.46975801, 1670219.73633286, 1298000.32853234,\n       1071816.48763003, 1372181.62763932, 1110194.55030423,\n        785093.52551906,  922088.97580721, 1104112.97959792,\n       1045094.09149729, 1510914.43292827, 1324413.85967803,\n       1569955.52697379, 1139994.3653257 , 1075623.0061941 ,\n        990756.03185819,  992439.03122436, 1696750.63264082,\n       1318006.76285044, 1626187.933248  , 1426841.27819474,\n       1238804.35095047, 1504419.18829708, 1699266.34457266,\n       1516403.58037732,  823754.72489952, 1718965.96252973,\n       1132175.57113273, 1497435.86874145,  934566.08165603,\n       1404337.49856037,  833930.84531217, 1185517.91615953,\n       1137422.32691787, 1322326.03358801, 1328598.39291966,\n       1528798.31010231, 1251061.97480092, 1478382.66847569,\n       1271181.61519816, 1181671.00795775,  916755.99759565,\n       1667101.8086411 , 1823141.88304839, 1088766.55036998,\n       1257198.52749067, 1328706.83995574, 1369157.36444634,\n        973773.60532907,  703281.64934846, 1515168.12040324,\n       1058388.63493951, 1100412.65920713, 1571329.26156108,\n       1526625.52116458, 1039311.95992991, 1803256.8864758 ,\n       1389081.11893812,  968759.52611896, 1602473.98355629,\n       1251565.44144685,  947575.7067101 ,  766625.16003246,\n       1548865.24072949, 1120554.73471455,  965421.29771347,\n       1694943.07073609,  440246.04888594, 1599152.52365688,\n       1175039.16348192,  599186.87193643,  443720.89859505,\n       1335364.9500612 ,  736810.12204184,  625366.48968576,\n       1025958.0144829 , 1520345.541842  , 1343907.61039133,\n       1264936.70882119, 1371208.11417684,  722215.8726605 ,\n       1079371.3900645 , 1294007.91136026,  743784.67144427,\n       1189309.75095576, 1026592.13397411, 1441411.85198903,\n       1496561.51218369,  551672.89931317, 1386277.60682014,\n       1288357.23397635, 1748218.72796802, 2129786.55033017,\n        935205.34305878, 1426401.76829747,  905306.60850858,\n       1838658.54335181, 1628560.73228968, 1555319.07508468,\n       1283947.54924084, 1753750.76512505, 1244582.34553525,\n        916787.01526364, 1583277.35767455,  755251.24369461,\n       1309057.54037239, 1221265.71523061, 1531966.88768046,\n       1767331.86970217, 1549524.22565959, 1580398.53562736,\n       1510756.99361088, 1668286.48973643, 1153430.71356206,\n        694592.76450065, 1007658.30492915, 1251134.09307986,\n       1612176.5388557 , 1331905.85147741, 1089863.15527629,\n       1581259.1048612 , 1353486.76998345, 1395505.45808154,\n       1431597.12772088, 1625164.89589177, 1241082.70759863,\n        832032.92920501, 1447457.26970895, 1696617.40100163,\n       1112195.41683316, 1715175.34825836, 1195063.90682076,\n       1852318.83574631,  936635.18760823, 1028696.56392351,\n       1119482.8937946 , 1376907.082805  , 1473338.71938086,\n       1007033.6437037 ,  945710.19123809, 1298372.30292334,\n       1464551.30629951, 1366805.32138598,  695397.53164451,\n       1666714.22921793, 1092320.40494978, 1927481.20271321,\n       1165099.60544502, 1353280.90287523, 1537148.98263965,\n       1054865.49794515, 1863022.22100039, 1357616.30038073,\n        780240.53892283,  820259.75953404, 1756092.40944391,\n       1069525.8452014 , 1886993.14774088, 1640739.73917004,\n       1072868.11444634, 1908836.21362323,  913304.60271711,\n       1637058.28033329, 1620725.74638773, 1518201.03458117,\n        675020.75358518,  750489.45024119,  670513.32137024,\n        890069.88532508, 1079293.00928632,  900757.34759356,\n       1231084.80873501,  928928.14000682,  540697.93586043,\n       1214581.12276504, 1060648.00956728, 1468472.98733865,\n       1267050.9050933 , 1444347.47340969, 1298583.62154773,\n       1182751.67449413, 1225192.19553724, 1369808.22651958,\n       1735952.21706243, 1330761.92174518, 1401983.4653229 ,\n       1374735.39681172,  846853.94001597, 1296920.95170962,\n       1434683.66803428, 1379886.52458325,  884040.96160374,\n        939716.49596716, 1385172.17071937, 1551427.01294039,\n        850068.04330501, 1037728.55757721, 1543402.62423474,\n       1183398.90023382, 1179786.34329167, 1298039.56314124,\n       1113232.0054411 ,  651285.07422711, 1844724.02989167,\n       1222843.10956696,  709704.91489566, 1425019.48845976,\n       1302389.50821194, 1231006.9209219 , 1100675.87692524,\n        519905.61215199, 1139563.72365057, 1709847.15307406,\n        794691.53270288, 1323279.47018335, 1547192.79926255,\n       1545442.18168166, 1705844.16070799, 1213240.68177948,\n       1125876.73765904,  987306.64191981, 1153634.69047416,\n       1632883.71802887, 1594767.69351641, 1557369.94199846,\n       1131574.52011857,  780018.79279261, 1582654.12352348,\n       1665474.42140505, 1258285.7926694 , 1213198.61603706,\n       1556269.80381034, 1708903.5562422 ,  511685.7353109 ,\n       1158928.7506833 , 1436699.63369589, 1160784.95248269,\n        646809.57445903, 1263409.590189  , 1615867.19822629,\n       1444052.90345073, 1245513.17045302, 1881644.95095632,\n        742794.69710161, 1821767.71986905,  972490.35391992,\n       1745336.61597522, 1684563.93598837, 1271467.72545913,\n        713605.28584346, 1803498.15207833,  546916.68710786,\n       2133963.53238436,  826671.06654446, 1746568.13238849,\n       1352572.43978754, 1176782.36273691, 1190416.55511831,\n       1685474.62338483, 1133354.66218544, 1024539.16273622,\n       1020240.20079351, 1140374.9001646 ,  988626.76785416,\n       1639609.52285014, 1211900.55934517,  506355.61367646,\n       1388829.11368997, 1125541.4053822 , 1332515.373456  ,\n       1038096.51802182, 1257810.39279464, 1445917.42431142,\n       1638908.3211578 , 1124085.54006953,  767890.62072627,\n        771326.07475694, 1336286.16086596, 1510764.03375743,\n       1379300.75804599, 1489696.69958357, 1022120.16319164,\n       2206778.93847448, 1479768.23912311, 1445757.86750574,\n       1024539.64361395,  557318.39633288, 1934852.383704  ,\n       1140716.44336162, 1253958.67446122, 1196109.53585469,\n       1069236.5089541 , 1622856.82734513, 1510187.57345142,\n       1320870.93458449, 1278282.19670119, 1648058.33881166,\n        793767.20258811, 1096615.99566774, 1281570.11915354,\n       1715176.22511885, 1299060.21888255, 1441561.91661068,\n       1277283.15409267, 1124936.01337588, 1686621.0073143 ,\n       1375792.44948378,  647005.84741187, 1497110.9553054 ,\n       1234016.55435995, 1305669.1212948 , 1112598.16899793,\n       1210350.32829168,  786156.44805102,  928545.55260778,\n       1252055.40123858, 1486800.49937222,  692309.75213451,\n       1220637.71222384, 1055303.32897129,  788599.61025446,\n        672873.29552813, 1005980.51299439,  924211.37399429,\n       1338995.98139896, 1168706.35896656,  980349.40684183,\n       1386795.47634582, 1460106.00224785, 1640748.14307618,\n       1517058.63571583, 1432469.20595009, 1434050.30775427,\n       1242784.30128716, 1260031.51507551, 1655671.90963323,\n       1297327.11377151, 1376919.9486493 , 1207704.89355024,\n       1343453.96473877, 1663625.53894606, 1547313.7957561 ,\n       1041200.04146295, 1350065.01957771, 1526459.03486008,\n       1480506.95026577, 1137348.16336584, 1532592.86030661,\n       1487778.44726158,  453750.85766677, 1296745.05908101,\n       1660802.42385959, 1213524.33262713,  927758.19951474,\n       1256331.51815562, 1336045.00652904, 1422897.2456305 ,\n        923713.31387929, 1704845.25894618, 1880369.02915361,\n        937841.72425503, 1460847.44714513, 1308009.57414803,\n       1412899.90417064, 1120977.83753404,  999858.70775036,\n       1366428.1429109 , 1212717.49483415,  944293.23399615,\n        436029.13508741, 1201217.34587968, 1391705.92629568,\n       1184220.9598942 , 1313579.43281585, 1339780.36620466,\n        990228.37386416,  982073.94336228, 1336608.5346129 ,\n        554217.25870977, 1559010.98000369, 1089279.63038502,\n       1265108.87116156, 1018601.26388139, 1052985.33174305,\n        708319.74551772,  816716.33738387,  809175.20356678,\n       1587160.60958689,  818351.77872206, 1362390.21873845,\n        969371.03058399, 1008977.09329486, 1259566.67610309,\n       1259960.69141919,  915294.28377223, 1011726.06790268,\n       1386258.75550418, 1687525.40167641, 1261538.08532018,\n        647998.18215832, 1439358.85855398, 1252662.83626916,\n       1316560.0451681 , 1087708.98084187, 1355004.50134313,\n       1106893.46209223, 1165553.50216046,  638291.49362318,\n       1251842.10775256, 1496072.30549298,  980873.18889513,\n       1304648.20658986, 1526250.23655531, 1123402.1327012 ,\n       1494359.0418606 ,  888000.68843743, 1393921.58630247,\n       1280164.67967675, 1082179.46792108, 1225557.91903473,\n       1121184.56783934, 1024335.94518954, 1417999.00054183,\n        883621.58615544, 1086751.96664457, 1372910.61253913,\n        793677.6011    , 1211785.95180862, 1028370.87109449,\n       1291572.5543836 , 1245166.65184532, 1247132.14631612,\n       1290150.68427318, 1733440.40569813, 1755244.08363444,\n       1183417.5342127 ,  962292.60405769, 1333772.71764106,\n       1198877.95555841, 1301307.18310465, 1691927.12440598,\n        694115.93027361, 1644850.48866729,  929847.22722478,\n        740460.5792097 , 1627730.34748143, 1382742.10412989,\n       1199412.42435387, 1876931.75885951, 1250731.53207634,\n       1380216.36778365, 1672575.33572692,  998659.89871926,\n       1241459.31047081, 1305951.22820401,  895839.88041653,\n        977245.91228177,  845164.24054763, 1403885.20003137,\n       1162743.23528572, 1289147.21704123, 1735928.04690299,\n       1520226.70595486, 1721114.31067663, 1106044.8492373 ,\n       1070144.81609184,  958413.61321527,  826093.91894925,\n        905031.3216307 , 1277447.44400802, 1061488.20493434,\n       1454262.51950168, 1044016.40551973,  792568.63697706,\n       1611776.20719911, 1319302.63909008, 1176271.90453299,\n       1199157.91080234, 1229776.13962206, 1190401.83042905,\n       1542135.02735538, 1291252.20247452, 1214491.24084312,\n        771260.38702555, 1125925.27843561,  623521.55934871,\n       1100596.07351085, 1485280.70958613, 1425384.00116102,\n       1072107.19265435, 1484941.14152175, 1375320.19149652,\n       1285997.53197097,  976164.73141636, 1236378.53654495,\n       1000943.23178996,  951603.18352775, 1090022.4483327 ,\n        691580.40034408,  848761.36957086, 1082730.96523662,\n       1049705.54435848, 1108972.49297767, 1417174.25085797,\n       1065121.1693258 , 1472979.36804813, 1182607.68143583,\n       1051685.16162639, 1262099.51684329, 1406541.17334631,\n        785472.86277019,  742635.64344748,  972358.59357155,\n       1503417.05053728, 1426353.00437632, 1153631.75598698,\n       1201391.60410432, 1493056.13243117, 1597464.13864538,\n       1682771.12463953, 1821428.16822834,  987718.62468692,\n       1048320.88222431, 1178957.87548537, 1209673.15659013,\n       1455806.78062196,  859938.07761026, 1222286.59600566,\n       1213794.92191853, 1811991.64843313, 1879192.79700536,\n       1095539.71199302, 1276739.44841956, 1358899.32425151,\n       1456880.60010002, 1518976.88779526,  607972.48783317,\n       1510943.7523716 , 1033565.87329021,  946129.49979784,\n       1354612.47391972, 1941930.07602066,  923774.27692776,\n       1492894.76848646,  644063.51181365,  984190.65381265,\n       1337486.704503  , 1424275.65734742, 1164134.69909659,\n       1486847.80527135, 1237033.46779537, 1650294.07886712,\n       1219731.08453762, 1495645.58840586, 1390567.86206284,\n       1046620.91557622, 1150470.95299574, 1387538.67531422,\n       1558209.76637303, 1738338.27884055, 1458394.27852758,\n       1231257.29511059, 1109216.98813045, 1073740.4289437 ,\n       1498893.16319495, 1396234.5342056 ,  953134.69497543,\n       1294648.25010262, 1146771.47226296, 1335105.43863206,\n        663493.38559999, 1441331.97805724, 1331891.71190404,\n       1299033.88470275, 1975126.22408591, 1818382.10791015,\n        635589.48221648, 1479830.36090672, 1251296.66755207,\n        715280.94952336, 1098524.25805448, 1461373.43975043,\n        706141.07301947, 1127301.46649572, 1023105.90008338,\n       1050734.95853498, 1566604.21516465, 1074159.47770152,\n       1045965.33182746, 1001915.95912831, 1161789.91738571,\n        920386.95258564, 1505233.23122978, 1810478.13712799,\n        966697.90710214, 1467392.17177148, 1315573.19580944,\n       1168166.09134923,  981919.12934661,  505869.70310167,\n       1321576.3882892 , 1263828.74192069, 1547821.7490337 ,\n       1093896.14293131, 1076725.47573584,  871832.39973515,\n        582536.51979667, 1353129.59847977, 1033212.84626559,\n       1655804.45362884, 1151805.64492535, 1825432.79754343,\n       1535635.85895225, 1198260.68220522, 1550520.12723863,\n       1509872.76313403, 1582320.41059096, 1269227.07562042,\n       1408279.60971935, 1041368.30922355, 1240041.57291559,\n        997565.46111357, 1510175.51480209, 1688691.72115688,\n       1168876.76620471,  789988.38448383, 1236437.91795691,\n        960600.13982779, 1311921.86602909,  842764.66159854,\n       1250360.4454846 ,  921755.77173495,  979023.42603998,\n       1320920.11302833, 1157390.29244178, 1760302.05465615,\n       1190466.90613259,  969093.28130915, 1291815.29221764,\n       1672072.15755103,  999837.42699397, 1215611.18161059,\n       1404577.651046  ,  613681.58858071, 1317411.54867951,\n        823970.1542631 , 1703125.73597044,  933926.9488354 ,\n       1137199.11570783,  569106.75765795, 1294760.10708948,\n       1442229.8051445 , 1299043.65584492, 1239000.82268083,\n       1112113.02653811,  635992.23609497, 1114512.1414425 ,\n       1533755.53347822, 1019643.54017362, 1088661.74733259,\n        836884.26828541, 1124744.15465077, 1607839.24877131,\n       1500724.55772223, 1372533.34057233, 1169884.69622041,\n        672600.1024945 , 1516098.33502597,  822541.67498559,\n       1078960.47305747, 1564991.53373413, 1162001.98207495,\n       1508595.08900596, 1369457.08559567, 1585945.04445205,\n       1047803.19420643, 1235300.5713675 , 1379814.76584822,\n       1843564.29705726, 1549915.09221874, 1508040.10466767,\n       1445716.26931433, 1256082.87650148, 1334222.80760887,\n       1427332.481704  , 1530265.78769085,  910402.952559  ,\n       1283852.56186099,  384697.71039027, 1154197.05190291,\n       1109331.67390934,  895133.48540413,  768958.85105874,\n        995397.44077764, 1074790.15405513, 1165239.71259339,\n       1550506.52977733, 1373561.48646322, 1110224.71216156,\n        835818.95043312, 1692370.04026318, 1478513.46775942,\n       2099774.2919203 , 1757439.78976485, 1125723.88028233,\n       1921037.99168822, 1196852.66905287,  636102.39598178,\n        698470.20494719, 1742072.19852749,  671795.57616343,\n       1544436.48415413,  899799.53513011,  772357.64346097,\n       1429871.6125083 , 1427106.74669545, 1047644.73263996,\n       1733663.78182278,  967709.28403967, 1543674.67570352,\n       1156571.65528593, 1689771.04431276, 1411641.46254891,\n       1557051.55295783,  985252.43256089, 1048446.93604087,\n       1297283.05784117,  953776.34889693, 1311954.07949727,\n       1189291.01597781, 1515380.50678299,  924803.32870949,\n       1151284.74152266, 1903171.51302312, 1197912.81381722,\n       1355414.4763576 , 1267579.88513761, 1551459.04186057,\n       1053368.56341298, 1614366.85544942, 1321291.06981738,\n       1207895.67061824,  584500.79977459,  798782.9281815 ,\n       1122639.48684174, 1200576.28724881, 1272681.22605966,\n       1173721.45732602, 1309969.09869378,  998374.09009731,\n       1111945.97977748, 1457354.42393614, 1144483.63811768,\n       1351194.61423752, 1496731.8972288 ,  850270.59741054,\n       1376807.78365303, 1576719.79671261, 1062181.7881776 ,\n       1446943.62934719, 1224425.17851874, 1097672.64170982,\n       1790248.1791866 , 1059085.89933225,  936432.87872338,\n       1250625.36813987, 1457019.46102888, 1560278.1419132 ,\n       1521250.42157252, 1021886.83303455,  530918.12672372,\n        691182.16401806, 1127214.87269592, 1201382.72654434,\n       1289121.72936825,  567779.4618981 , 1186462.9056098 ,\n       1166458.71643715, 1723639.85084698, 1825451.88107403,\n       1092665.52567372, 1464660.97374309, 1513299.44488687,\n       1353035.4961232 , 1405555.55833118, 1224655.57766815,\n       1598646.51033456, 1233592.53185168,  814555.85413014,\n       1284230.56083975, 1113398.7775722 , 1775237.19448459,\n       1518068.51139801,  744353.21639094, 1243976.22983412,\n        893306.43690813, 1694804.55894976, 1520492.6093133 ,\n       1528811.75484656, 1335095.89012784, 1391493.84172329,\n        509577.4474123 , 1580885.72142141, 2065547.93786857,\n        437736.77298564, 1421122.01804909, 1139476.62597959,\n       1491232.68818872,  947998.81536448,  833290.9102628 ,\n        869229.12420945,  545038.85695363,  831319.66574739,\n       1064147.68607189, 1490886.79511337, 1291260.18310726,\n       1227935.24788198,  965674.21694242, 1634760.28898396,\n       1219018.54950976, 1334172.10575499,  704207.95800352,\n       1103637.69329153, 1480288.80308928, 1213185.61834909,\n       1288142.86110647, 1357916.71184034, 1179298.95696165,\n       1484887.37732938, 1312331.16366007, 1494174.74498657,\n       1253684.1178251 ,  720660.68110816, 1609584.56248623,\n       1346814.0851155 , 1202122.1385215 , 1427869.13295058,\n       1160659.17241874, 1004539.14791287, 1204112.54136867,\n       1926191.84993978,  737646.18008295, 1004696.48404246,\n       1403797.89891372, 1665981.04058064,  978050.98854534,\n       1002607.02764009,  742310.14918893,  947154.51521608,\n       1234955.92195921, 1266372.65182822, 1604045.02305096,\n       1551308.93946315,  748981.40995205, 1743795.6311031 ,\n       1580590.54570719, 1679982.66437647,  978406.35098172,\n       1396018.89637615,  987251.63387184, 1645605.87363061,\n        803480.48987292, 1563927.18764507, 1427183.8269756 ,\n       1700047.5795575 , 1223450.68687352, 1126632.11998896,\n       1328268.67590097, 1223564.42485499, 1787891.83053872,\n        778080.40463326, 1393337.78782582, 1699428.47764452,\n       1475425.84466384,  902602.88479088, 1286003.80459218,\n       1114376.40207513, 1033160.99217017, 1386145.15863088,\n       1125488.04201556, 1219248.39032372, 1049646.66406838,\n       1428381.40891242, 1473071.36825804, 1528192.36642981,\n       1369066.6218251 , 1051910.92716404, 1640078.45263021,\n       1179802.63721317,  922559.03700383, 1050695.21122495,\n        642061.91258325, 1484432.20582464, 1005138.37523761,\n       1696149.58548116, 1046488.59567603, 1506293.51396936,\n       1590683.75285865, 2170473.40230055, 1306398.27916771,\n        988120.36745217, 1239853.6198666 , 1590251.27139417,\n        532564.12306392, 1631209.91930741, 1190692.08126216,\n       1503015.51283295,  841846.72240476, 1579646.64685102,\n        774231.63515775, 1451967.92193129, 1040263.11164952,\n       1377714.77334669, 1207329.43648081, 1116591.39632574,\n        588422.90871709, 1575545.90369605, 1384736.98133995,\n       1299753.36006892, 1159030.91220454, 1072933.47102213,\n       1378324.49116345,  986255.23683569, 1369259.24476021,\n       1180018.8288868 ,  820829.79826084, 1538479.2641172 ,\n        852583.54693737, 1572688.18103696, 1229028.06648855,\n        693146.69181082,  624346.81638169, 1693013.40713185,\n       1602361.55517646,  847532.85189675,  896260.45335112,\n       1279894.36297805, 1152116.13793517,  849509.65983618,\n        997752.84992442, 1169479.83747297,  996180.59286588,\n       1170709.04933571, 1507162.51516198, 1296514.35197962,\n        767725.9039988 , 1040434.68014385,  916540.69559751,\n       1210336.76335647, 1086602.12475376, 1035424.24794159,\n       1269983.34093118, 1340736.99662848, 1049967.66527995,\n       1411492.65808439, 1065642.73020297, 1252372.14711936,\n       1294056.71243587, 1520166.90732574, 1640092.4523033 ,\n       1252134.87795471,  923210.04611275, 1255060.47617587,\n        953026.17232814, 1100925.7163729 , 1238527.530081  ,\n        967295.56075016, 1057493.05929803, 1151852.97481984,\n       1607624.564354  ,  693229.10591577, 1935320.18051293,\n       1275536.28965597,  757686.64566102, 1228637.89949307,\n       1231871.13980906, 1023131.1600205 ,  893775.9180784 ,\n       1677583.03861489, 1606062.01654804, 1609032.35434288,\n       1308376.87923087, 1187021.23432525, 1120192.44368507,\n       1296582.0510369 , 1886259.27031041, 1716252.91874913,\n       1054673.4716054 ])\n\n\n\n\nXGBoost Regressor\n\n# Define the parameter grid\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [3, 6, 9],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'subsample': [0.7, 0.8, 0.9],\n    'colsample_bytree': [0.7, 0.8, 0.9]\n}\n\n# Initialize XGBoost regressor\nxgb_reg = xgb.XGBRegressor(objective ='reg:squarederror', random_state=42)\n\n# Setup GridSearchCV\ngrid_search = GridSearchCV(estimator=xgb_reg, param_grid=param_grid, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n\n# Fit the model\ngrid_search.fit(X_train, y_train)\n\n# Get the best estimator\nbest_xgb = grid_search.best_estimator_\n\n\n\nXGBoost Regressor Model Evaluation\n\n# Make predictions using the best model\npredictions = best_xgb.predict(X_test)\n\n# Calculate and print evaluation metrics\nmse = mean_squared_error(y_test, predictions)\nr2 = r2_score(y_test, predictions)\nprint(f'Mean Squared Error: {mse}')\nprint(f'R-squared: {r2}')\nprint('Best Parameters:', grid_search.best_params_)\n\nMean Squared Error: 11064311070.076391\nR-squared: 0.9100699796059779\nBest Parameters: {'colsample_bytree': 0.7, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.9}\n\n\n\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, predictions)\n\nprint(f'Root Mean Squared Error: {rmse}')\nprint(f'Mean Absolute Error: {mae}')\n\nRoot Mean Squared Error: 107444.14620312591\nMean Absolute Error: 85927.5765781728\n\n\n\n\nElastic Net Model\n\n# Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Define the Elastic Net model\nelastic_net = ElasticNet(random_state=42)\n\n# Define the grid of parameters to search\nparam_grid = {\n    'alpha': [0.1, 1, 10, 100],\n    'l1_ratio': [0.1, 0.5, 0.9]\n}\n\n# Setup GridSearchCV\ngrid_search = GridSearchCV(estimator=elastic_net, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n\n# Fit the model\ngrid_search.fit(X_train_scaled, y_train)\n\n# Get the best estimator\nbest_elastic_net = grid_search.best_estimator_\n\n# Make predictions\npredictions = best_elastic_net.predict(X_test_scaled)\n\n\nMean Squared Error: 10100765132.277464\nR2 Score: 0.9179016200296809\nBest Parameters: {'alpha': 0.1, 'l1_ratio': 0.9}\n\n\n\n\nElastic Net Model Evaluation\n\n# Calculate MSE\nmse = mean_squared_error(y_test, predictions)\nr2 = r2_score(y_test, predictions)\nprint(f'Mean Squared Error: {mse}')\nprint(f'R2 Score: {r2}')\nprint('Best Parameters:', grid_search.best_params_)\n\nMean Squared Error: 11544244553.318697\nR2 Score: 0.9061691106171728\nBest Parameters: {'alpha': 0.1, 'l1_ratio': 0.9}"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This project was developed by MineCrafters For INFO 523 - Data Mining and Discovery at the University of Arizona, taught by Dr. Greg Chism. The team is comprised of the following team members.\n\nAslam Sheik Dawood: Graduate student in Data Science, University of Arizona\nHari Dave: Graduate student in Data Science, University of Arizona\nPartha Koundinya P: Graduate student in Data Science, University of Arizona\nSarthak Miglani: Graduate student in Data Science, University of Arizona\nVinu Kevin Diesel: Graduate student in Data Science, University of Arizona"
  },
  {
    "objectID": "MineCrafters.html",
    "href": "MineCrafters.html",
    "title": "Brick-Metrics",
    "section": "",
    "text": "This project is aimed at the development and evaluation of regression models for predicting housing prices using a dataset titled ‘USA_Housing.csv.’ The overarching goal is to construct robust and accurate models through the application of various machine learning algorithms. The project follows a systematic workflow, encompassing data exploration, feature engineering, model training, hyperparameter tuning, and comprehensive performance evaluation.\nThe initial phase involves the importing of essential Python libraries, such as NumPy, Pandas, Matplotlib, Seaborn, and scikit-learn, conducting an in-depth exploratory data analysis (EDA), unveiling insights into data distributions, correlations, and potential preprocessing requirements. Visualizations, including histograms, bar plots, and heatmaps, aid in understanding the dataset’s characteristics.\nFurther, the project employs a diverse set of regression models, including Decision Tree Regressor, Random Forest Regressor, XGBoost Regressor, Support Vector Regression (SVR), and Elastic Net Regression. Thus, the project provides a practical implementation of regression modeling for housing price prediction"
  },
  {
    "objectID": "MineCrafters.html#abstract",
    "href": "MineCrafters.html#abstract",
    "title": "Brick-Metrics",
    "section": "",
    "text": "This project is aimed at the development and evaluation of regression models for predicting housing prices using a dataset titled ‘USA_Housing.csv.’ The overarching goal is to construct robust and accurate models through the application of various machine learning algorithms. The project follows a systematic workflow, encompassing data exploration, feature engineering, model training, hyperparameter tuning, and comprehensive performance evaluation.\nThe initial phase involves the importing of essential Python libraries, such as NumPy, Pandas, Matplotlib, Seaborn, and scikit-learn, conducting an in-depth exploratory data analysis (EDA), unveiling insights into data distributions, correlations, and potential preprocessing requirements. Visualizations, including histograms, bar plots, and heatmaps, aid in understanding the dataset’s characteristics.\nFurther, the project employs a diverse set of regression models, including Decision Tree Regressor, Random Forest Regressor, XGBoost Regressor, Support Vector Regression (SVR), and Elastic Net Regression. Thus, the project provides a practical implementation of regression modeling for housing price prediction"
  },
  {
    "objectID": "MineCrafters.html#introduction",
    "href": "MineCrafters.html#introduction",
    "title": "Brick-Metrics",
    "section": "Introduction",
    "text": "Introduction\nIn the dynamic landscape of the US real estate market, predicting house prices is paramount for informed decision-making. This project employs cutting-edge machine learning techniques to unravel the intricate patterns and factors influencing housing values. By leveraging vast datasets, our aim is to develop a robust predictive model."
  },
  {
    "objectID": "MineCrafters.html#question",
    "href": "MineCrafters.html#question",
    "title": "Brick-Metrics",
    "section": "Question",
    "text": "Question\nHow accurately can the housing prices in the US be predicted?"
  },
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "Brick-Metrics",
    "section": "",
    "text": "The primary objective of this project is to develop a predictive model for housing prices in the US.\nThis project centers around utilizing the USA_Housing dataset to build a predictive model for housing prices. The project aims to create a robust and accurate model that can estimate housing prices for given sets of input variables by employing regression algorithms. By leveraging machine learning techniques, the goal is to analyze the relationships between various features such as Average Area Income, Average Area House Age, Average Area of Rooms, Average Area of bedrooms, and Area Population, and predict the housing prices based on these factors.\nThrough exploratory data analysis, feature engineering, and machine learning modeling, the project will uncover patterns, correlations, and dependencies within the dataset. Subsequently, a regression model will be trained to understand and quantify the influence of each feature on the housing prices. The model’s performance will be evaluated using metrics such as mean squared error or R-squared, ensuring its reliability and accuracy."
  },
  {
    "objectID": "proposal.html#project-objective",
    "href": "proposal.html#project-objective",
    "title": "Brick-Metrics",
    "section": "",
    "text": "The primary objective of this project is to develop a predictive model for housing prices in the US.\nThis project centers around utilizing the USA_Housing dataset to build a predictive model for housing prices. The project aims to create a robust and accurate model that can estimate housing prices for given sets of input variables by employing regression algorithms. By leveraging machine learning techniques, the goal is to analyze the relationships between various features such as Average Area Income, Average Area House Age, Average Area of Rooms, Average Area of bedrooms, and Area Population, and predict the housing prices based on these factors.\nThrough exploratory data analysis, feature engineering, and machine learning modeling, the project will uncover patterns, correlations, and dependencies within the dataset. Subsequently, a regression model will be trained to understand and quantify the influence of each feature on the housing prices. The model’s performance will be evaluated using metrics such as mean squared error or R-squared, ensuring its reliability and accuracy."
  },
  {
    "objectID": "proposal.html#dataset",
    "href": "proposal.html#dataset",
    "title": "Brick-Metrics",
    "section": "Dataset",
    "text": "Dataset\n\n\nCode\n#&gt; Installing necessary libraries \n\nlibrary(tidyverse)\n\n#&gt; Loading the Dataset - solemate\n\nbrickMetrics &lt;- read.csv(\"data/USA_Housing.csv\")\n\n\nThe dataset comes from the Kaggle Database. The dataset consists of 5000 observations related to houses accross the United States with various features of information including their Average Area Income, Average Area House Age, Average Area Number of Rooms, Average Area Number of Bedrooms, Area Population,Address, and Price With this sizeable and diverse sample of housing data, this dataset promises to serve as a foundational resource for building the Brick-Metrics system. The table representing the features in the dataset and their description is presented below.\n\n\n\n\n\n\n\n\nFeature\nData Type\nDescription\n\n\n\n\nAvg. Area Income\nInteger\nAverage Area Income of the population in the specific location\n\n\nAvg. Area House Age\nInteger\nAverage Area House Age\n\n\nAvg. Area Number of Rooms\nInteger\nAverage Area of total Number of Rooms\n\n\nAverage Area Number of Rooms\nInteger\nAverage Area of total Number of Bedrooms\n\n\nArea Population\nInteger\nPopulation of the place\n\n\nAddress\nCharacter\nAddress of the house\n\n\nPrice\nInteger\nPrice of the house"
  },
  {
    "objectID": "proposal.html#question",
    "href": "proposal.html#question",
    "title": "Brick-Metrics",
    "section": "Question",
    "text": "Question\nHow accurately can the housing prices in the US be predicted?"
  },
  {
    "objectID": "proposal.html#analysis-plan",
    "href": "proposal.html#analysis-plan",
    "title": "Brick-Metrics",
    "section": "Analysis plan",
    "text": "Analysis plan\nBy following the below analysis plan, the project can efficiently implement a robust recommendation system that enhances the development of the prediction system.\n\nExploratory Data Analysis (EDA):\n\nAcquire and load the dataset.\nExplore the dataset to understand its structure and the variables it contains.\nVisualize data distributions, identify outliers, and check for missing values.\nAnalyze customer reviews and ratings to gain insights into user preferences.\n\nData Preprocessing:\n\nClean the data by addressing missing values and outliers.\nTransform categorical variables into numerical representations (e.g., one-hot encoding for brand, material, and style).\nSplit the data into training and testing sets for model evaluation.\n\nData Visualization\n\nVisualizing the necessary correlations.\n\nFeature Engineering:\n\nCreate additional features if necessary.\n\nTraining and Testing:\n\nSelect machine learning algorithms suitable for prediction like regression models.\nTrain multiple models on the training data.\nEvaluate model performance using testing data.\n\nModel Evaluation:\n\nUtilize metrics like Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and accuracy to assess each model’s performance.\nCompare the results to determine which models are the most effective in making recommendations.\n\nChoosing the Best Model:\n\nSelect the model with the lowest error or highest accuracy, depending on the chosen evaluation metric."
  },
  {
    "objectID": "proposal.html#plan-of-attack",
    "href": "proposal.html#plan-of-attack",
    "title": "Brick-Metrics",
    "section": "Plan of Attack",
    "text": "Plan of Attack\n\n\n\n\n\n\n\n\nWeek\nWeekly Tasks\nTeam members involved\n\n\n\n\nTill November 8th\nExplore and finalize the dataset and the problem statements\nEveryone\n\n\n-\nComplete the proposal and assign some high-level tasks\nEveryone\n\n\nNov. 9th - 15th\nExploratory Data Analysis\nHari, Syed\n\n\n-\nData cleaning and Data pre-processing based on EDA\nPartha, Vinu, Sarthak\n\n\n-\nQuestion specific exploration and identify initial trends and patterns\nSarthak, Syed\n\n\nNov. 16th - 22nd\nData Preprocessing\nHari, Vinu\n\n\n-\nData Visualization\nPartha, Sarthak\n\n\nNov. 23rd - 29th\nFeature Engineering\nHari, Syed\n\n\n-\nTraining and Testing\nPartha, Vinu, Sarthak\n\n\nNov. 30th - December 6th\nRefining the code for code review with comments\nSarthak, Syed\n\n\n-\nModel Evaluation\nHari, Partha\n\n\nDec. 7th - 11th\nChoosing the Best Model\nEveryone\n\n\n-\nWrite-up and presentation for the project\nEveryone"
  },
  {
    "objectID": "proposal.html#repo-organization",
    "href": "proposal.html#repo-organization",
    "title": "Brick-Metrics",
    "section": "Repo Organization",
    "text": "Repo Organization\nThe following are the folders involved in the Project repository.\n\ndata/: Used for storing any necessary data files for the project, such as input files.\nimages/: Used for storing image files used in the project.\npresentation_files/: Folder for having presentation related files.\n_extra/: Used to brainstorm our analysis which won’t impact our project workflow.\n_freeze/: This folder is used to store the generated files during the build process. These files represent the frozen state of the website at a specific point in time.\n_site/: Folder used to store the generated static website files after the site generator processes the quarto document.\n.github/: Folder for storing github templates and workflow.\nsrc/: Folder used to store the source code file."
  },
  {
    "objectID": "MineCrafters.html#approach",
    "href": "MineCrafters.html#approach",
    "title": "Brick-Metrics",
    "section": "Approach",
    "text": "Approach\nInitially, the ‘USA_Housing.csv’ dataset was loaded, and a meticulous Exploratory Data Analysis (EDA) was performed. The intricacies of data types, null values, and statistical summaries were unveiled, providing critical insights into the dataset’s characteristics. Then, it was split into training and testing sets to develop the machine learning models. Further, the regression were developed and were evaluated on the basis of R2 score."
  },
  {
    "objectID": "MineCrafters.html#analysis",
    "href": "MineCrafters.html#analysis",
    "title": "Brick-Metrics",
    "section": "Analysis",
    "text": "Analysis\nThis section presents the complete process of the developing the Decision Tree, Random Forest, XGBoost Regressor, Support Vector Regression, and Elastic Net Regression techniques.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\n# Reading csv file\ndata = pd.read_csv('USA_Housing.csv')\ndata\n\n\n\n\n\n\n\n\n\nAvg. Area Income\nAvg. Area House Age\nAvg. Area Number of Rooms\nAvg. Area Number of Bedrooms\nArea Population\nPrice\nAddress\n\n\n\n\n0\n79545.458574\n5.682861\n7.009188\n4.09\n23086.800503\n1.059034e+06\n208 Michael Ferry Apt. 674\\r\\nLaurabury, NE 37...\n\n\n1\n79248.642455\n6.002900\n6.730821\n3.09\n40173.072174\n1.505891e+06\n188 Johnson Views Suite 079\\r\\nLake Kathleen, ...\n\n\n2\n61287.067179\n5.865890\n8.512727\n5.13\n36882.159400\n1.058988e+06\n9127 Elizabeth Stravenue\\r\\nDanieltown, WI 064...\n\n\n3\n63345.240046\n7.188236\n5.586729\n3.26\n34310.242831\n1.260617e+06\nUSS Barnett\\r\\nFPO AP 44820\n\n\n4\n59982.197226\n5.040555\n7.839388\n4.23\n26354.109472\n6.309435e+05\nUSNS Raymond\\r\\nFPO AE 09386\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4995\n60567.944140\n7.830362\n6.137356\n3.46\n22837.361035\n1.060194e+06\nUSNS Williams\\r\\nFPO AP 30153-7653\n\n\n4996\n78491.275435\n6.999135\n6.576763\n4.02\n25616.115489\n1.482618e+06\nPSC 9258, Box 8489\\r\\nAPO AA 42991-3352\n\n\n4997\n63390.686886\n7.250591\n4.805081\n2.13\n33266.145490\n1.030730e+06\n4215 Tracy Garden Suite 076\\r\\nJoshualand, VA ...\n\n\n4998\n68001.331235\n5.534388\n7.130144\n5.44\n42625.620156\n1.198657e+06\nUSS Wallace\\r\\nFPO AE 73316\n\n\n4999\n65510.581804\n5.992305\n6.792336\n4.07\n46501.283803\n1.298950e+06\n37778 George Ridges Apt. 509\\r\\nEast Holly, NV...\n\n\n\n\n5000 rows × 7 columns\n\n\n\n\nExploratory Data Analysis\n\n\nCode\ndata.info()#print a concise summary of a DataFrame\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5000 entries, 0 to 4999\nData columns (total 7 columns):\n #   Column                        Non-Null Count  Dtype  \n---  ------                        --------------  -----  \n 0   Avg. Area Income              5000 non-null   float64\n 1   Avg. Area House Age           5000 non-null   float64\n 2   Avg. Area Number of Rooms     5000 non-null   float64\n 3   Avg. Area Number of Bedrooms  5000 non-null   float64\n 4   Area Population               5000 non-null   float64\n 5   Price                         5000 non-null   float64\n 6   Address                       5000 non-null   object \ndtypes: float64(6), object(1)\nmemory usage: 273.6+ KB\n\n\n\n\nCode\n# Checking null values\ndata.isnull().sum()\n\n\nAvg. Area Income                0\nAvg. Area House Age             0\nAvg. Area Number of Rooms       0\nAvg. Area Number of Bedrooms    0\nArea Population                 0\nPrice                           0\nAddress                         0\ndtype: int64\n\n\n\n\nCode\ndata.describe()# Generate descriptive statistics summarizing central tendency, dispersion, and shape of dataset's distribution.\n\n\n\n\n\n\n\n\n\nAvg. Area Income\nAvg. Area House Age\nAvg. Area Number of Rooms\nAvg. Area Number of Bedrooms\nArea Population\nPrice\n\n\n\n\ncount\n5000.000000\n5000.000000\n5000.000000\n5000.000000\n5000.000000\n5.000000e+03\n\n\nmean\n68583.108984\n5.977222\n6.987792\n3.981330\n36163.516039\n1.232073e+06\n\n\nstd\n10657.991214\n0.991456\n1.005833\n1.234137\n9925.650114\n3.531176e+05\n\n\nmin\n17796.631190\n2.644304\n3.236194\n2.000000\n172.610686\n1.593866e+04\n\n\n25%\n61480.562388\n5.322283\n6.299250\n3.140000\n29403.928702\n9.975771e+05\n\n\n50%\n68804.286404\n5.970429\n7.002902\n4.050000\n36199.406689\n1.232669e+06\n\n\n75%\n75783.338666\n6.650808\n7.665871\n4.490000\n42861.290769\n1.471210e+06\n\n\nmax\n107701.748378\n9.519088\n10.759588\n6.500000\n69621.713378\n2.469066e+06\n\n\n\n\n\n\n\n\n\nCode\n# Data Claening\ndf = pd.DataFrame()\ndf['Income'] = data['Avg. Area Income'].round(2)# Round 'Avg. Area Income' to 2 decimal places and store in 'Income'.\ndf['House Age'] = data['Avg. Area House Age'].apply(int)# Converting 'Avg. Area House Age' to integers and adding it to 'df' as 'House Age'.\ndf['No. of Rooms'] = data['Avg. Area Number of Rooms'].apply(int)# Converting 'Avg. Area Number of Rooms' to integers and adding it to 'df' as 'No. of Rooms'.\ndf['No. of Bedrooms'] = data['Avg. Area Number of Bedrooms'].apply(int)# Converting 'Avg. Area Number of Bedrooms' to integers and adding it to 'df' as 'No. of Bedrooms'.\ndf['Population'] = data['Area Population'].apply(int)# Converting 'Area Population' to integers and adding it to 'df' as 'Population'.\ndf['Price'] = data['Price'].apply(int)# Converting 'Price' to integers and adding it to 'df' as 'Price'.\n\n\n\n\nCode\nsns.displot(data['Avg. Area Number of Bedrooms'], kde=False, bins=40)\n\n\nC:\\Users\\Jithin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\axisgrid.py:123: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Price plot\nplt.figure(figsize = (12, 6), dpi = 200)\nsns.distplot(data.Price)\n\n\nC:\\Users\\Jithin\\AppData\\Local\\Temp\\ipykernel_28276\\3496450648.py:3: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(data.Price)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Price wrt Income\nplt.figure(figsize = (12, 6), dpi = 200)\nsns.lineplot(x = 'Income', y = 'Price', data = df)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Price wrt House Age\nplt.figure(figsize = (12, 6), dpi = 200)\nsns.barplot(x = 'House Age', y = 'Price', data = df, ci = None)\n\n\nC:\\Users\\Jithin\\AppData\\Local\\Temp\\ipykernel_28276\\1414414798.py:3: FutureWarning: \n\nThe `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n\n  sns.barplot(x = 'House Age', y = 'Price', data = df, ci = None)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Price wrt No. of Rooms\nplt.figure(figsize = (12, 6), dpi = 200)\nsns.lineplot(x = 'No. of Rooms', y = 'Price', data = df)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Price wrt No. of Bedrooms\nplt.figure(figsize = (12, 6), dpi = 200)\nsns.barplot(x = 'No. of Bedrooms', y = 'Price', data = df, ci = None)\n\n\nC:\\Users\\Jithin\\AppData\\Local\\Temp\\ipykernel_28276\\139840174.py:3: FutureWarning: \n\nThe `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n\n  sns.barplot(x = 'No. of Bedrooms', y = 'Price', data = df, ci = None)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Price wrt Population\nplt.figure(figsize = (12, 6), dpi = 200)\nsns.scatterplot(x = 'Population', y = 'Price', data = df)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef categorize_house_age(df):\n    max_age = df['Avg. Area House Age'].max()\n    bins = [0, 5, 10, max_age]\n    labels = ['New', 'Mid-Age', 'Old']\n    if max_age &lt;= 10:  # Adjusting bins if max age is less than or equal to 10\n        bins = [0, max_age / 3, 2 * max_age / 3, max_age]\n    df['House_Age_Category'] = pd.cut(df['Avg. Area House Age'], bins=bins, labels=labels, include_lowest=True)\n    return df\n\ndata = categorize_house_age(data)\n\n\n\n\nCode\n# Pairplot segmented by a new categorical feature (if applicable)\nsns.pairplot(data, hue='House_Age_Category')\nplt.show()\n\n\nC:\\Users\\Jithin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\axisgrid.py:123: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndata\n\n\n\n\n\n\n\n\n\nAvg. Area Income\nAvg. Area House Age\nAvg. Area Number of Rooms\nAvg. Area Number of Bedrooms\nArea Population\nPrice\nAddress\nHouse_Age_Category\n\n\n\n\n0\n79545.458574\n5.682861\n7.009188\n4.09\n23086.800503\n1.059034e+06\n208 Michael Ferry Apt. 674\\r\\nLaurabury, NE 37...\nMid-Age\n\n\n1\n79248.642455\n6.002900\n6.730821\n3.09\n40173.072174\n1.505891e+06\n188 Johnson Views Suite 079\\r\\nLake Kathleen, ...\nMid-Age\n\n\n2\n61287.067179\n5.865890\n8.512727\n5.13\n36882.159400\n1.058988e+06\n9127 Elizabeth Stravenue\\r\\nDanieltown, WI 064...\nMid-Age\n\n\n3\n63345.240046\n7.188236\n5.586729\n3.26\n34310.242831\n1.260617e+06\nUSS Barnett\\r\\nFPO AP 44820\nOld\n\n\n4\n59982.197226\n5.040555\n7.839388\n4.23\n26354.109472\n6.309435e+05\nUSNS Raymond\\r\\nFPO AE 09386\nMid-Age\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4995\n60567.944140\n7.830362\n6.137356\n3.46\n22837.361035\n1.060194e+06\nUSNS Williams\\r\\nFPO AP 30153-7653\nOld\n\n\n4996\n78491.275435\n6.999135\n6.576763\n4.02\n25616.115489\n1.482618e+06\nPSC 9258, Box 8489\\r\\nAPO AA 42991-3352\nOld\n\n\n4997\n63390.686886\n7.250591\n4.805081\n2.13\n33266.145490\n1.030730e+06\n4215 Tracy Garden Suite 076\\r\\nJoshualand, VA ...\nOld\n\n\n4998\n68001.331235\n5.534388\n7.130144\n5.44\n42625.620156\n1.198657e+06\nUSS Wallace\\r\\nFPO AE 73316\nMid-Age\n\n\n4999\n65510.581804\n5.992305\n6.792336\n4.07\n46501.283803\n1.298950e+06\n37778 George Ridges Apt. 509\\r\\nEast Holly, NV...\nMid-Age\n\n\n\n\n5000 rows × 8 columns\n\n\n\n\n\nCode\n# Boxplot for Prices across different House Age Categories\nsns.barplot(x='House_Age_Category', y='Price', data=data)\nplt.title('House Prices across Age Categories')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# One-Hot Encoding for categorical variables\ndata = pd.get_dummies(data, columns=['House_Age_Category'])\n\n\n\n\nCode\ndata\n\n\n\n\n\n\n\n\n\nAvg. Area Income\nAvg. Area House Age\nAvg. Area Number of Rooms\nAvg. Area Number of Bedrooms\nArea Population\nPrice\nAddress\nHouse_Age_Category_New\nHouse_Age_Category_Mid-Age\nHouse_Age_Category_Old\n\n\n\n\n0\n79545.458574\n5.682861\n7.009188\n4.09\n23086.800503\n1.059034e+06\n208 Michael Ferry Apt. 674\\r\\nLaurabury, NE 37...\nFalse\nTrue\nFalse\n\n\n1\n79248.642455\n6.002900\n6.730821\n3.09\n40173.072174\n1.505891e+06\n188 Johnson Views Suite 079\\r\\nLake Kathleen, ...\nFalse\nTrue\nFalse\n\n\n2\n61287.067179\n5.865890\n8.512727\n5.13\n36882.159400\n1.058988e+06\n9127 Elizabeth Stravenue\\r\\nDanieltown, WI 064...\nFalse\nTrue\nFalse\n\n\n3\n63345.240046\n7.188236\n5.586729\n3.26\n34310.242831\n1.260617e+06\nUSS Barnett\\r\\nFPO AP 44820\nFalse\nFalse\nTrue\n\n\n4\n59982.197226\n5.040555\n7.839388\n4.23\n26354.109472\n6.309435e+05\nUSNS Raymond\\r\\nFPO AE 09386\nFalse\nTrue\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4995\n60567.944140\n7.830362\n6.137356\n3.46\n22837.361035\n1.060194e+06\nUSNS Williams\\r\\nFPO AP 30153-7653\nFalse\nFalse\nTrue\n\n\n4996\n78491.275435\n6.999135\n6.576763\n4.02\n25616.115489\n1.482618e+06\nPSC 9258, Box 8489\\r\\nAPO AA 42991-3352\nFalse\nFalse\nTrue\n\n\n4997\n63390.686886\n7.250591\n4.805081\n2.13\n33266.145490\n1.030730e+06\n4215 Tracy Garden Suite 076\\r\\nJoshualand, VA ...\nFalse\nFalse\nTrue\n\n\n4998\n68001.331235\n5.534388\n7.130144\n5.44\n42625.620156\n1.198657e+06\nUSS Wallace\\r\\nFPO AE 73316\nFalse\nTrue\nFalse\n\n\n4999\n65510.581804\n5.992305\n6.792336\n4.07\n46501.283803\n1.298950e+06\n37778 George Ridges Apt. 509\\r\\nEast Holly, NV...\nFalse\nTrue\nFalse\n\n\n\n\n5000 rows × 10 columns\n\n\n\n\n\nSetting Target and Variable and Train Test Split\n\n\nCode\n# Selecting features and target variable\nX = data.drop(['Price', 'Address'], axis=1)\ny = data['Price']\n\n\n\n\nCode\n# Splitting the dataset into training and testing sets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\n\nSupport Vector Regression\n\n\nCode\npip install --upgrade tensorflow\n\n\nRequirement already satisfied: tensorflow in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.15.0)\nRequirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (2.15.0)\nRequirement already satisfied: absl-py&gt;=1.0.0 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0-&gt;tensorflow) (2.0.0)\nRequirement already satisfied: astunparse&gt;=1.6.0 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0-&gt;tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers&gt;=23.5.26 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0-&gt;tensorflow) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,&gt;=0.2.1 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0-&gt;tensorflow) (0.5.4)\nRequirement already satisfied: google-pasta&gt;=0.1.1 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0-&gt;tensorflow) (0.2.0)\nRequirement already satisfied: h5py&gt;=2.9.0 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0-&gt;tensorflow) (3.10.0)\nRequirement already satisfied: libclang&gt;=13.0.0 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0-&gt;tensorflow) (16.0.6)\nRequirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0-&gt;tensorflow) (0.2.0)\nRequirement already satisfied: numpy&lt;2.0.0,&gt;=1.23.5 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0-&gt;tensorflow) (1.25.2)\nRequirement already satisfied: opt-einsum&gt;=2.3.2 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0-&gt;tensorflow) (3.3.0)\nRequirement already satisfied: packaging in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0-&gt;tensorflow) (23.1)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,&lt;5.0.0dev,&gt;=3.20.3 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0-&gt;tensorflow) (4.23.4)\nRequirement already satisfied: setuptools in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0-&gt;tensorflow) (65.5.0)\nRequirement already satisfied: six&gt;=1.12.0 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0-&gt;tensorflow) (1.16.0)\nRequirement already satisfied: termcolor&gt;=1.1.0 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0-&gt;tensorflow) (2.4.0)\nRequirement already satisfied: typing-extensions&gt;=3.6.6 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0-&gt;tensorflow) (4.8.0)\nRequirement already satisfied: wrapt&lt;1.15,&gt;=1.11.0 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0-&gt;tensorflow) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem&gt;=0.23.1 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0-&gt;tensorflow) (0.31.0)\nRequirement already satisfied: grpcio&lt;2.0,&gt;=1.24.3 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0-&gt;tensorflow) (1.58.0)\nRequirement already satisfied: tensorboard&lt;2.16,&gt;=2.15 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0-&gt;tensorflow) (2.15.1)\nRequirement already satisfied: tensorflow-estimator&lt;2.16,&gt;=2.15.0 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0-&gt;tensorflow) (2.15.0)\nRequirement already satisfied: keras&lt;2.16,&gt;=2.15.0 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.15.0-&gt;tensorflow) (2.15.0)\nRequirement already satisfied: wheel&lt;1.0,&gt;=0.23.0 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from astunparse&gt;=1.6.0-&gt;tensorflow-intel==2.15.0-&gt;tensorflow) (0.42.0)\nRequirement already satisfied: google-auth&lt;3,&gt;=1.6.3 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard&lt;2.16,&gt;=2.15-&gt;tensorflow-intel==2.15.0-&gt;tensorflow) (2.25.2)\nRequirement already satisfied: google-auth-oauthlib&lt;2,&gt;=0.5 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard&lt;2.16,&gt;=2.15-&gt;tensorflow-intel==2.15.0-&gt;tensorflow) (1.1.0)\nRequirement already satisfied: markdown&gt;=2.6.8 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard&lt;2.16,&gt;=2.15-&gt;tensorflow-intel==2.15.0-&gt;tensorflow) (3.5.1)\nRequirement already satisfied: requests&lt;3,&gt;=2.21.0 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard&lt;2.16,&gt;=2.15-&gt;tensorflow-intel==2.15.0-&gt;tensorflow) (2.31.0)\nRequirement already satisfied: tensorboard-data-server&lt;0.8.0,&gt;=0.7.0 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard&lt;2.16,&gt;=2.15-&gt;tensorflow-intel==2.15.0-&gt;tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug&gt;=1.0.1 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard&lt;2.16,&gt;=2.15-&gt;tensorflow-intel==2.15.0-&gt;tensorflow) (3.0.1)\nRequirement already satisfied: cachetools&lt;6.0,&gt;=2.0.0 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard&lt;2.16,&gt;=2.15-&gt;tensorflow-intel==2.15.0-&gt;tensorflow) (5.3.2)\nRequirement already satisfied: pyasn1-modules&gt;=0.2.1 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard&lt;2.16,&gt;=2.15-&gt;tensorflow-intel==2.15.0-&gt;tensorflow) (0.3.0)\nRequirement already satisfied: rsa&lt;5,&gt;=3.1.4 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard&lt;2.16,&gt;=2.15-&gt;tensorflow-intel==2.15.0-&gt;tensorflow) (4.9)\nRequirement already satisfied: requests-oauthlib&gt;=0.7.0 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth-oauthlib&lt;2,&gt;=0.5-&gt;tensorboard&lt;2.16,&gt;=2.15-&gt;tensorflow-intel==2.15.0-&gt;tensorflow) (1.3.1)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.16,&gt;=2.15-&gt;tensorflow-intel==2.15.0-&gt;tensorflow) (3.2.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.16,&gt;=2.15-&gt;tensorflow-intel==2.15.0-&gt;tensorflow) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.16,&gt;=2.15-&gt;tensorflow-intel==2.15.0-&gt;tensorflow) (1.26.16)\nRequirement already satisfied: certifi&gt;=2017.4.17 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;2.16,&gt;=2.15-&gt;tensorflow-intel==2.15.0-&gt;tensorflow) (2023.7.22)\nRequirement already satisfied: MarkupSafe&gt;=2.1.1 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from werkzeug&gt;=1.0.1-&gt;tensorboard&lt;2.16,&gt;=2.15-&gt;tensorflow-intel==2.15.0-&gt;tensorflow) (2.1.3)\nRequirement already satisfied: pyasn1&lt;0.6.0,&gt;=0.4.6 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard&lt;2.16,&gt;=2.15-&gt;tensorflow-intel==2.15.0-&gt;tensorflow) (0.5.1)\nRequirement already satisfied: oauthlib&gt;=3.0.0 in c:\\users\\jithin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;2,&gt;=0.5-&gt;tensorboard&lt;2.16,&gt;=2.15-&gt;tensorflow-intel==2.15.0-&gt;tensorflow) (3.2.2)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\nCode\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\n\nWARNING:tensorflow:From C:\\Users\\Jithin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n\n\n\n\n\nNeural Network\n\n\nCode\n# Without Hyperparameter tuning\n# Data preprocessing: Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Neural network model architecture\nmodel = Sequential()\nmodel.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(1))  # Output layer for regression\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mean_squared_error')\n\n# Train the model\nhistory = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n\n# Evaluate the model on test data\npredictions = model.predict(X_test_scaled)\nmse = mean_squared_error(y_test, predictions)\nr2 = r2_score(y_test, predictions)\nprint(f'Mean Squared Error: {mse}')\nprint(f'R-squared: {r2}')\n\n\nWARNING:tensorflow:From C:\\Users\\Jithin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\nWARNING:tensorflow:From C:\\Users\\Jithin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\nWARNING:tensorflow:From C:\\Users\\Jithin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n\n 1/32 [..............................] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 504us/step\nMean Squared Error: 30448872247.9731\nR-squared: 0.7525134927161543\n\n\n\n\nCode\n# Define the function to create and train the Keras model\ndef create_model(neurons=64, activation='relu', optimizer='adam'):\n    model = Sequential()\n    model.add(Dense(neurons, activation=activation, input_shape=(X_train.shape[1],)))\n    model.add(Dense(32, activation=activation))\n    model.add(Dense(1))  # Output layer for regression\n    model.compile(optimizer=optimizer, loss='mean_squared_error')\n    return model\n\n# Define hyperparameters to search through\nneurons_list = [32, 64, 128]\nactivation_list = ['relu', 'tanh']\noptimizer_list = ['adam', 'rmsprop']\nbatch_size_list = [32, 64]\nepochs_list = [50, 100]\n\nbest_score = float('inf')\nbest_params = {}\n\n# Iterate through different hyperparameter combinations\nfor neurons in neurons_list:\n    for activation in activation_list:\n        for optimizer in optimizer_list:\n            for batch_size in batch_size_list:\n                for epochs in epochs_list:\n                    # Create and train the model\n                    model = create_model(neurons=neurons, activation=activation, optimizer=optimizer)\n                    model.fit(X_train_scaled, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n                    \n                    # Evaluate the model\n                    predictions = model.predict(X_test_scaled)\n                    mse = mean_squared_error(y_test, predictions)\n                    \n                    # Update best parameters if a better model is found\n                    if mse &lt; best_score:\n                        best_score = mse\n                        best_params = {\n                            'neurons': neurons,\n                            'activation': activation,\n                            'optimizer': optimizer,\n                            'batch_size': batch_size,\n                            'epochs': epochs\n                        }\n\n# Train the best model using the best parameters\nbest_model = create_model(neurons=best_params['neurons'],\n                          activation=best_params['activation'],\n                          optimizer=best_params['optimizer'])\nbest_model.fit(X_train_scaled, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'], verbose=0)\n\n# Evaluate the best model on test data\npredictions = best_model.predict(X_test_scaled)\nmse = mean_squared_error(y_test, predictions)\nr2 = r2_score(y_test, predictions)\nprint(f'Mean Squared Error: {mse}')\nprint(f'R-squared: {r2}')\nprint('Best Parameters:', best_params)\n\n\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 504us/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 503us/step\n 1/32 [..............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 1ms/step\n 1/32 [..............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 1ms/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 1ms/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 504us/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 503us/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 504us/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 708us/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 503us/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 503us/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 1ms/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 689us/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 503us/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 503us/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 1ms/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 503us/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 503us/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 503us/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 504us/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 1ms/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 584us/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 843us/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 321us/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 503us/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 1ms/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 503us/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 662us/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 503us/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 593us/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 503us/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 1ms/step\n 1/32 [..............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 1ms/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 1ms/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 503us/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 504us/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 1ms/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 1ms/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 211us/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 504us/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 504us/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 503us/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 504us/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 864us/step\n 1/32 [..............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 1ms/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 504us/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 504us/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 504us/step\n 1/32 [..............................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/32 [==============================] - 0s 1ms/step\nMean Squared Error: 15682320765.157217\nR-squared: 0.8725350889627105\nBest Parameters: {'neurons': 128, 'activation': 'relu', 'optimizer': 'adam', 'batch_size': 32, 'epochs': 100}\n\n\n\n\nCode\n# Define the model\nmodel = SVR()\n\n# Define the parameters for GridSearchCV\nparam_grid = {\n    'C': [0.1, 1, 10, 100],  # Regularization parameter\n    'gamma': ['scale', 'auto'],  # Kernel coefficient\n    'kernel': ['linear', 'sigmoid']  # Type of kernel\n}\n\n# Create GridSearchCV\ngrid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1)\n\n# Perform grid search\ngrid_search.fit(X_train, y_train)\n\n# Print results\nprint(\"Best parameters found: \", grid_search.best_params_)\nprint(\"Best score found: \", grid_search.best_score_)\n\n\nBest parameters found:  {'C': 100, 'gamma': 'scale', 'kernel': 'linear'}\nBest score found:  0.73778443791727\n\n\n\n\nSupport Vector Regressor Model Evaluation\n\n\nCode\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Get the best estimator\nbest_svr = grid_search.best_estimator_\n\n# Make predictions using the best model on the scaled test data\npredictions = best_svr.predict(X_test)\n\n# Calculate and print evaluation metrics\nmse = mean_squared_error(y_test, predictions)\nr2 = r2_score(y_test, predictions)\nprint(f'Mean Squared Error: {mse}')\nprint(f'R-squared: {r2}')\nprint('Best Parameters:', grid_search.best_params_)\n\n\nMean Squared Error: 32693943451.0945\nR-squared: 0.7342656960115967\nBest Parameters: {'C': 100, 'gamma': 'scale', 'kernel': 'linear'}\n\n\n\n\nCode\n#cross validation for model evaluation\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.svm import SVR\nimport numpy as np\n\nsvr_scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\nprint(\"SVR Mean Squared Error:\", np.mean(svr_scores))\n\n\nSVR Mean Squared Error: -124677012216.41104\n\n\n\n\nCode\n#plot actual and predicted values\nsns.scatterplot(x=y_test, y=predictions)\nplt.xlabel('True Values')\nplt.ylabel('Predicted Values')\n\n\nText(0, 0.5, 'Predicted Values')\n\n\n\n\n\n\n\n\n\n\n\nRandom Forest Regressor\n\n\nCode\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n# Define the parameter grid for hyperparameter tuning\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [5, 10, 15, 20],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# Initialize the Random Forest model\nrf = RandomForestRegressor(random_state=42)\n\n# Setup GridSearchCV\ngrid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n\n# Fit the model to the scaled training data\ngrid_search.fit(X_train, y_train)\n\n# Get the best estimator\nbest_rf = grid_search.best_estimator_\n\n\n\n\nRandom Forest Model Evaluation\n\n\nCode\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Make predictions using the best model on the scaled test data\npredictions = best_rf.predict(X_test)\n\n# Calculate and print evaluation metrics\nmse = mean_squared_error(y_test, predictions)\nr2 = r2_score(y_test, predictions)\nprint(f'Mean Squared Error: {mse}')\nprint(f'R-squared: {r2}')\nprint('Best Parameters:', grid_search.best_params_)\n\n\nMean Squared Error: 14280727311.341614\nR-squared: 0.8839271518835241\nBest Parameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 300}\n\n\n\n\nCode\n#cross validation for model evaluation\nrf_scores = cross_val_score(best_rf, X, y, cv=5, scoring='neg_mean_squared_error')\n\nprint(\"Random Forest Mean Squared Error:\", np.mean(rf_scores))\n\n\nRandom Forest Mean Squared Error: -14504544972.249462\n\n\n\n\nCode\n#plot actual and predicted values\nsns.scatterplot(x=y_test, y=predictions)\nplt.xlabel('True Values')\nplt.ylabel('Predicted Values')\n\n\nText(0, 0.5, 'Predicted Values')\n\n\n\n\n\n\n\n\n\n\n\nXGBoost Regressor\n\n\nCode\n# Define the parameter grid\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [3, 6, 9],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'subsample': [0.7, 0.8, 0.9],\n    'colsample_bytree': [0.7, 0.8, 0.9]\n}\n\n# Initialize XGBoost regressor\nxgb_reg = xgb.XGBRegressor(objective ='reg:squarederror', random_state=42)\n\n# Setup GridSearchCV\ngrid_search = GridSearchCV(estimator=xgb_reg, param_grid=param_grid, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n\n# Fit the model\ngrid_search.fit(X_train, y_train)\n\n# Get the best estimator\nbest_xgb = grid_search.best_estimator_\n\n\n\n\nXGBoost Regressor Model Evaluation\n\n\nCode\n# Make predictions using the best model\npredictions = best_xgb.predict(X_test)\n\n# Calculate and print evaluation metrics\nmse = mean_squared_error(y_test, predictions)\nr2 = r2_score(y_test, predictions)\nprint(f'Mean Squared Error: {mse}')\nprint(f'R-squared: {r2}')\nprint('Best Parameters:', grid_search.best_params_)\n\n\nMean Squared Error: 10946017250.64358\nR-squared: 0.9110314642864701\nBest Parameters: {'colsample_bytree': 0.7, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}\n\n\n\n\nCode\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, predictions)\n\nprint(f'Root Mean Squared Error: {rmse}')\nprint(f'Mean Absolute Error: {mae}')\n\n\nRoot Mean Squared Error: 104623.21563899468\nMean Absolute Error: 83344.45606738135\n\n\n\n\nCode\n#cross validation for model evaluation\nxgb_scores = cross_val_score(best_xgb, X, y, cv=5, scoring='neg_mean_squared_error')\nprint(\"XGBoost Mean Squared Error:\", np.mean(xgb_scores))\n\n\nXGBoost Mean Squared Error: -11628096775.104847\n\n\n\n\nCode\n#plot actual and predicted values\nsns.scatterplot(x=y_test, y=predictions)\nplt.xlabel('True Values')\nplt.ylabel('Predicted Values')\n\n\nText(0, 0.5, 'Predicted Values')\n\n\n\n\n\n\n\n\n\n\n\nElastic Net Model\n\n\nCode\n# Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Define the Elastic Net model\nelastic_net = ElasticNet(random_state=42)\n\n# Define the grid of parameters to search\nparam_grid = {\n    'alpha': [0.1, 1, 10, 100],\n    'l1_ratio': [0.1, 0.5, 0.9]\n}\n\n# Setup GridSearchCV\ngrid_search = GridSearchCV(estimator=elastic_net, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n\n# Fit the model\ngrid_search.fit(X_train_scaled, y_train)\n\n# Get the best estimator\nbest_elastic_net = grid_search.best_estimator_\n\n# Make predictions\npredictions = best_elastic_net.predict(X_test_scaled)\n\n\n\n\nElastic Net Model Evaluation\n\n\nCode\n# Calculate MSE\nmse = mean_squared_error(y_test, predictions)\nr2 = r2_score(y_test, predictions)\nprint(f'Mean Squared Error: {mse}')\nprint(f'R2 Score: {r2}')\nprint('Best Parameters:', grid_search.best_params_)\n\n\nMean Squared Error: 10097788698.683523\nR2 Score: 0.9179258122936284\nBest Parameters: {'alpha': 0.1, 'l1_ratio': 0.9}\n\n\n\n\nCode\n#cross validation for model evaluation\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n\nen_scores = cross_val_score(best_elastic_net, X_scaled, y, cv=5, scoring='neg_mean_squared_error')\nprint(\"ElasticNet Mean Squared Error:\", np.mean(en_scores))\n\n\nElasticNet Mean Squared Error: -10277827255.660938\n\n\n\n\nCode\n#plot actual and predicted values\nsns.scatterplot(x=y_test, y=predictions)\nplt.xlabel('True Values')\nplt.ylabel('Predicted Values')\n\n\nText(0, 0.5, 'Predicted Values')"
  },
  {
    "objectID": "MineCrafters.html#discussion",
    "href": "MineCrafters.html#discussion",
    "title": "Brick-Metrics",
    "section": "Discussion",
    "text": "Discussion\n\n\n\nModel\nR-squared Value\n\n\n\n\nNeural Network\n0.8714\n\n\nSupport Vector\n0.7655\n\n\nRandom Forest\n0.8839\n\n\nXGBoost Regression\n0.9105\n\n\nElastic Net Model\n0.9179\n\n\n\nThe regression models exhibit varying degrees of predictive performance, with the Elastic Net model demonstrating the highest R-squared value (0.9179), closely followed by XGBoost Regression (0.9105), Random Forest Regression (0.8839), Neural Network (0.8714), and Support Vector Regression (0.7655).\nHence, the Elastic Net model could be considered for the prediction of the house prices."
  }
]