{
  "hash": "7c89a2c048ed9443b1fecbd297d5883d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Brick-Metrics\"\nsubtitle: \"Building Precision in Predicting USA Home Prices\"\nformat: html\ncode-fold: true\neditor: visual\n---\n\n## Project Objective\n\nThe primary objective of this project is to develop a predictive model for housing prices in the US.  \n\nThis project centers around utilizing the USA_Housing dataset to build a predictive model for housing prices. The project aims to create a robust and accurate model that can estimate housing prices for given sets of input variables by employing regression algorithms. By leveraging machine learning techniques, the goal is to analyze the relationships between various features such as Average Area Income, Average Area House Age, Average Area of Rooms, Average Area of bedrooms, and Area Population, and predict the housing prices based on these factors. \n\nThrough exploratory data analysis, feature engineering, and machine learning modeling, the project will uncover patterns, correlations, and dependencies within the dataset. Subsequently, a regression model will be trained to understand and quantify the influence of each feature on the housing prices. The model's performance will be evaluated using metrics such as mean squared error or R-squared, ensuring its reliability and accuracy.\n\n\n\n\n## Dataset\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#> Installing necessary libraries \n\nlibrary(tidyverse)\n\n#> Loading the Dataset - solemate\n\nbrickMetrics <- read.csv(\"data/USA_Housing.csv\")\n```\n:::\n\n\nThe dataset comes from the [Kaggle Database](https://www.kaggle.com/datasets/aariyan101/usa-housingcsv/data). The dataset consists of 5000 observations related to houses accross the United States with various features of information including their `Average Area Income`, `Average Area House Age`, `Average Area Number of Rooms`, `Average Area Number of Bedrooms`, `Area Population`,`Address`, and `Price`\nWith this sizeable and diverse sample of housing data, this dataset promises to serve as a foundational resource for building the **Brick-Metrics** system. The table representing the features in the dataset and their description is presented below.\n\n\n| Feature       | Data Type | Description                                         |\n|---------------|---------------|------------------------------------------|\n| Avg. Area Income  | Integer | Average Area Income of the population in the specific location     |\n| Avg. Area House Age    | Integer | Average Area House Age                                  |\n| Avg. Area Number of Rooms | Integer   | Average Area of total Number of Rooms             |\n| Average Area Number of Rooms  | Integer   | Average Area of total Number of Bedrooms        |\n| Area Population      | Integer   | Population of the place          |\n| Address        | Character | Address of the house                               |\n| Price  | Integer | Price of the house                         |\n\n## Question\n\nHow accurately can the housing prices in the US be predicted?\n\n## Analysis plan\n\nBy following the below analysis plan, the project can efficiently implement a robust recommendation system that enhances the development of the prediction system.\n\n-   Exploratory Data Analysis (EDA):\n\n    -   Acquire and load the dataset.\n    -   Explore the dataset to understand its structure and the variables it contains.\n    -   Visualize data distributions, identify outliers, and check for missing values.\n    -   Analyze customer reviews and ratings to gain insights into user preferences.\n\n-   Data Preprocessing:\n\n    -   Clean the data by addressing missing values and outliers.\n    -   Transform categorical variables into numerical representations (e.g., one-hot encoding for brand, material, and style).\n    -   Split the data into training and testing sets for model evaluation.\n\n-   Data Visualization\n\n    -   Visualizing the necessary correlations.\n\n-   Feature Engineering:\n\n    -   Create additional features if necessary.\n\n-   Training and Testing:\n\n    -   Select machine learning algorithms suitable for prediction like regression models.\n    -   Train multiple models on the training data.\n    -   Evaluate model performance using testing data.\n\n-   Model Evaluation:\n\n    -   Utilize metrics like Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and accuracy to assess each model's performance.\n    -   Compare the results to determine which models are the most effective in making recommendations.\n\n-   Choosing the Best Model:\n\n    -   Select the model with the lowest error or highest accuracy, depending on the chosen evaluation metric.\n\n## Plan of Attack\n\n|             Week             |                              Weekly Tasks                              | Team members involved |\n|:---------------------:|:-------------------------:|:---------------------:|\n|     Till November 8^th^      |      Explore and finalize the dataset and the problem statements       |       Everyone        |\n|              \\-              |         Complete the proposal and assign some high-level tasks         |       Everyone        |\n|     Nov. 9^th^ - 15^th^      |                       Exploratory Data Analysis                        |      Hari, Syed       |\n|              \\-              |           Data cleaning and Data pre-processing based on EDA           | Partha, Vinu, Sarthak |\n|              \\-              | Question specific exploration and identify initial trends and patterns |     Sarthak, Syed     |\n|     Nov. 16^th^ - 22^nd^     |                           Data Preprocessing                           |      Hari, Vinu       |\n|              \\-              |                           Data Visualization                           |    Partha, Sarthak    |\n|     Nov. 23^rd^ - 29^th^     |                          Feature Engineering                           |      Hari, Syed       |\n|              \\-              |                          Training and Testing                          | Partha, Vinu, Sarthak |\n| Nov. 30^th^ - December 6^th^ |            Refining the code for code review with comments             |     Sarthak, Syed     |\n|              \\-              |                            Model Evaluation                            |     Hari, Partha      |\n|     Dec. 7^th^ - 11^th^      |                        Choosing the Best Model                         |       Everyone        |\n|              \\-              |               Write-up and presentation for the project                |       Everyone        |\n\n## Repo Organization\n\nThe following are the folders involved in the Project repository.\n\n-   **data/:** Used for storing any necessary data files for the project, such as input files.\n\n-   **images/:** Used for storing image files used in the project.\n\n-   **presentation_files/:** Folder for having presentation related files.\n\n-   **\\_extra/:** Used to brainstorm our analysis which won't impact our project workflow.\n\n-   **\\_freeze/:** This folder is used to store the generated files during the build process. These files represent the frozen state of the website at a specific point in time.\n\n-   **\\_site/:** Folder used to store the generated static website files after the site generator processes the quarto document.\n\n-   **.github/:** Folder for storing github templates and workflow.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}