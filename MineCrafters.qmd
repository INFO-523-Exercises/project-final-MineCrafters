---
title: Import Libraries and Load dataset
jupyter: python3
---



```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import OrdinalEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
from xgboost import XGBRegressor
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.linear_model import ElasticNet
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
```

```{python}
# Reading csv file
data = pd.read_csv('USA_Housing.csv')
data
```

# Exploratory Data Analysis

```{python}
data.info()#print a concise summary of a DataFrame
```

```{python}
# Checking null values
data.isnull().sum()
```

```{python}
data.describe()# Generate descriptive statistics summarizing central tendency, dispersion, and shape of dataset's distribution.
```

```{python}
# Data Claening
df = pd.DataFrame()
df['Income'] = data['Avg. Area Income'].round(2)# Round 'Avg. Area Income' to 2 decimal places and store in 'Income'.
df['House Age'] = data['Avg. Area House Age'].apply(int)# Converting 'Avg. Area House Age' to integers and adding it to 'df' as 'House Age'.
df['No. of Rooms'] = data['Avg. Area Number of Rooms'].apply(int)# Converting 'Avg. Area Number of Rooms' to integers and adding it to 'df' as 'No. of Rooms'.
df['No. of Bedrooms'] = data['Avg. Area Number of Bedrooms'].apply(int)# Converting 'Avg. Area Number of Bedrooms' to integers and adding it to 'df' as 'No. of Bedrooms'.
df['Population'] = data['Area Population'].apply(int)# Converting 'Area Population' to integers and adding it to 'df' as 'Population'.
df['Price'] = data['Price'].apply(int)# Converting 'Price' to integers and adding it to 'df' as 'Price'.
```

```{python}
sns.displot(data['Avg. Area Number of Bedrooms'], kde=False, bins=40)
```

```{python}
# Price plot
plt.figure(figsize = (12, 6), dpi = 200)
sns.distplot(data.Price)
```

```{python}
# Price wrt Income
plt.figure(figsize = (12, 6), dpi = 200)
sns.lineplot(x = 'Income', y = 'Price', data = df)
```

```{python}
# Price wrt House Age
plt.figure(figsize = (12, 6), dpi = 200)
sns.barplot(x = 'House Age', y = 'Price', data = df, ci = None)
```

```{python}
# Price wrt No. of Rooms
plt.figure(figsize = (12, 6), dpi = 200)
sns.lineplot(x = 'No. of Rooms', y = 'Price', data = df)
```

```{python}
# Price wrt No. of Bedrooms
plt.figure(figsize = (12, 6), dpi = 200)
sns.barplot(x = 'No. of Bedrooms', y = 'Price', data = df, ci = None)
```

```{python}
# Price wrt Population
plt.figure(figsize = (12, 6), dpi = 200)
sns.scatterplot(x = 'Population', y = 'Price', data = df)
```



```{python}
def categorize_house_age(df):
    max_age = df['Avg. Area House Age'].max()
    bins = [0, 5, 10, max_age]
    labels = ['New', 'Mid-Age', 'Old']
    if max_age <= 10:  # Adjusting bins if max age is less than or equal to 10
        bins = [0, max_age / 3, 2 * max_age / 3, max_age]
    df['House_Age_Category'] = pd.cut(df['Avg. Area House Age'], bins=bins, labels=labels, include_lowest=True)
    return df

data = categorize_house_age(data)
```

```{python}
# Pairplot segmented by a new categorical feature (if applicable)
sns.pairplot(data, hue='House_Age_Category')
plt.show()
```

```{python}
data
```

```{python}
# Boxplot for Prices across different House Age Categories
sns.barplot(x='House_Age_Category', y='Price', data=data)
plt.title('House Prices across Age Categories')
plt.show()
```

```{python}
# One-Hot Encoding for categorical variables
data = pd.get_dummies(data, columns=['House_Age_Category'])
```

```{python}
data
```


# Setting Target and Variable and Train Test Split

```{python}
# Selecting features and target variable
X = data.drop(['Price', 'Address'], axis=1)
y = data['Price']
```

```{python}
# Splitting the dataset into training and testing sets
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

# Support Vector Regression

```{python, eval = false}

pip install --upgrade tensorflow
```

```{python}
from sklearn.svm import SVR
from sklearn.ensemble import GradientBoostingRegressor
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
```

# Neural Network

```{python}
# Without Hyperparameter tuning
# Data preprocessing: Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Neural network model architecture
model = Sequential()
model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))
model.add(Dense(32, activation='relu'))
model.add(Dense(1))  # Output layer for regression

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)

# Evaluate the model on test data
predictions = model.predict(X_test_scaled)
mse = mean_squared_error(y_test, predictions)
r2 = r2_score(y_test, predictions)
print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')
```

```{python}
# Define the function to create and train the Keras model
def create_model(neurons=64, activation='relu', optimizer='adam'):
    model = Sequential()
    model.add(Dense(neurons, activation=activation, input_shape=(X_train.shape[1],)))
    model.add(Dense(32, activation=activation))
    model.add(Dense(1))  # Output layer for regression
    model.compile(optimizer=optimizer, loss='mean_squared_error')
    return model

# Define hyperparameters to search through
neurons_list = [32, 64, 128]
activation_list = ['relu', 'tanh']
optimizer_list = ['adam', 'rmsprop']
batch_size_list = [32, 64]
epochs_list = [50, 100]

best_score = float('inf')
best_params = {}

# Iterate through different hyperparameter combinations
for neurons in neurons_list:
    for activation in activation_list:
        for optimizer in optimizer_list:
            for batch_size in batch_size_list:
                for epochs in epochs_list:
                    # Create and train the model
                    model = create_model(neurons=neurons, activation=activation, optimizer=optimizer)
                    model.fit(X_train_scaled, y_train, epochs=epochs, batch_size=batch_size, verbose=0)
                    
                    # Evaluate the model
                    predictions = model.predict(X_test_scaled)
                    mse = mean_squared_error(y_test, predictions)
                    
                    # Update best parameters if a better model is found
                    if mse < best_score:
                        best_score = mse
                        best_params = {
                            'neurons': neurons,
                            'activation': activation,
                            'optimizer': optimizer,
                            'batch_size': batch_size,
                            'epochs': epochs
                        }

# Train the best model using the best parameters
best_model = create_model(neurons=best_params['neurons'],
                          activation=best_params['activation'],
                          optimizer=best_params['optimizer'])
best_model.fit(X_train_scaled, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'], verbose=0)

# Evaluate the best model on test data
predictions = best_model.predict(X_test_scaled)
mse = mean_squared_error(y_test, predictions)
r2 = r2_score(y_test, predictions)
print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')
print('Best Parameters:', best_params)
```

```{python}
# Define the model
model = SVR()

# Define the parameters for GridSearchCV
param_grid = {
    'C': [0.1, 1, 10, 100],  # Regularization parameter
    'gamma': ['scale', 'auto'],  # Kernel coefficient
    'kernel': ['linear', 'sigmoid']  # Type of kernel
}

# Create GridSearchCV
grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1)

# Perform grid search
grid_search.fit(X_train, y_train)

# Print results
print("Best parameters found: ", grid_search.best_params_)
print("Best score found: ", grid_search.best_score_)
```

# Support Vector Regressor Model Evaluation

```{python}
from sklearn.metrics import mean_squared_error, r2_score

# Get the best estimator
best_svr = grid_search.best_estimator_

# Make predictions using the best model on the scaled test data
predictions = best_svr.predict(X_test)

# Calculate and print evaluation metrics
mse = mean_squared_error(y_test, predictions)
r2 = r2_score(y_test, predictions)
print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')
print('Best Parameters:', grid_search.best_params_)
```

```{python}
#cross validation for model evaluation
from sklearn.model_selection import cross_val_score
from sklearn.svm import SVR
import numpy as np

svr_scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')
print("SVR Mean Squared Error:", np.mean(svr_scores))
```

```{python}
#plot actual and predicted values
sns.scatterplot(x=y_test, y=predictions)
plt.xlabel('True Values')
plt.ylabel('Predicted Values')
```

# Random Forest Regressor

```{python}
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [5, 10, 15, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize the Random Forest model
rf = RandomForestRegressor(random_state=42)

# Setup GridSearchCV
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')

# Fit the model to the scaled training data
grid_search.fit(X_train, y_train)

# Get the best estimator
best_rf = grid_search.best_estimator_
```

# Random Forest Model Evaluation

```{python}
from sklearn.metrics import mean_squared_error, r2_score

# Make predictions using the best model on the scaled test data
predictions = best_rf.predict(X_test)

# Calculate and print evaluation metrics
mse = mean_squared_error(y_test, predictions)
r2 = r2_score(y_test, predictions)
print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')
print('Best Parameters:', grid_search.best_params_)
```

```{python}
#cross validation for model evaluation
rf_scores = cross_val_score(best_rf, X, y, cv=5, scoring='neg_mean_squared_error')

print("Random Forest Mean Squared Error:", np.mean(rf_scores))
```

```{python}
#plot actual and predicted values
sns.scatterplot(x=y_test, y=predictions)
plt.xlabel('True Values')
plt.ylabel('Predicted Values')
```

# XGBoost Regressor

```{python}
# Define the parameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 6, 9],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.7, 0.8, 0.9],
    'colsample_bytree': [0.7, 0.8, 0.9]
}

# Initialize XGBoost regressor
xgb_reg = xgb.XGBRegressor(objective ='reg:squarederror', random_state=42)

# Setup GridSearchCV
grid_search = GridSearchCV(estimator=xgb_reg, param_grid=param_grid, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')

# Fit the model
grid_search.fit(X_train, y_train)

# Get the best estimator
best_xgb = grid_search.best_estimator_
```

# XGBoost Regressor Model Evaluation

```{python}
# Make predictions using the best model
predictions = best_xgb.predict(X_test)

# Calculate and print evaluation metrics
mse = mean_squared_error(y_test, predictions)
r2 = r2_score(y_test, predictions)
print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')
print('Best Parameters:', grid_search.best_params_)
```

```{python}
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, predictions)

print(f'Root Mean Squared Error: {rmse}')
print(f'Mean Absolute Error: {mae}')
```

```{python}
#cross validation for model evaluation
xgb_scores = cross_val_score(best_xgb, X, y, cv=5, scoring='neg_mean_squared_error')
print("XGBoost Mean Squared Error:", np.mean(xgb_scores))
```

```{python}
#plot actual and predicted values
sns.scatterplot(x=y_test, y=predictions)
plt.xlabel('True Values')
plt.ylabel('Predicted Values')
```

# Elastic Net Model

```{python}
# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define the Elastic Net model
elastic_net = ElasticNet(random_state=42)

# Define the grid of parameters to search
param_grid = {
    'alpha': [0.1, 1, 10, 100],
    'l1_ratio': [0.1, 0.5, 0.9]
}

# Setup GridSearchCV
grid_search = GridSearchCV(estimator=elastic_net, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')

# Fit the model
grid_search.fit(X_train_scaled, y_train)

# Get the best estimator
best_elastic_net = grid_search.best_estimator_

# Make predictions
predictions = best_elastic_net.predict(X_test_scaled)

```

# Elastic Net Model Evaluation

```{python}
# Calculate MSE
mse = mean_squared_error(y_test, predictions)
r2 = r2_score(y_test, predictions)
print(f'Mean Squared Error: {mse}')
print(f'R2 Score: {r2}')
print('Best Parameters:', grid_search.best_params_)
```

```{python}
#cross validation for model evaluation
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)


en_scores = cross_val_score(best_elastic_net, X_scaled, y, cv=5, scoring='neg_mean_squared_error')
print("ElasticNet Mean Squared Error:", np.mean(en_scores))
```

```{python}
#plot actual and predicted values
sns.scatterplot(x=y_test, y=predictions)
plt.xlabel('True Values')
plt.ylabel('Predicted Values')
```



